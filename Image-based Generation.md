# Image-based Generation

#### DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving

* **Authors**: Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, Jiwen Lu
* **Links**: [arXiv:2309.09777](https://arxiv.org/abs/2309.09777)

#### Drivedreamer-2: Llm-enhanced world models for diverse driving video generation

* **Authors**: Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, Xingang Wang
* **Links**: [arXiv:2403.06845](https://arxiv.org/abs/2403.06845)

#### DriveDreamer4D: Four-Dimensional Video Generation with World Model Priors for Autonomous Driving

* **Authors**: Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, Wenjun Mei, Xingang Wang
* **Links**: [arXiv:2410.13571](https://arxiv.org/abs/2410.13571)

#### ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration

* **Authors**:Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, Yifei Zhan, Kun Zhan, Peng Jia, Xianpeng Lang, Xingang Wang, Wenjun Mei
* **Links**: [arXiv:2411.19548](https://arxiv.org/abs/2411.19548)

#### WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens

* **Authors**: Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, Jiwen Lu
* **Links**: [arXiv:2401.09985](https://arxiv.org/abs/2401.09985)

#### CarDreamer: Open-Source Learning Platform for World Model based Autonomous Driving

* **Authors**: Dechen Gao, Shuangyu Cai, Hanchu Zhou, Hang Wang, Iman Soltani, Junshan Zhang
* **Links**: [arXiv:2405.09111](https://arxiv.org/abs/2405.09111)

#### BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout

* **Authors**: Kairui Yang, Enhui Ma, Jibin Peng, Qing Guo, Di Lin, Kaicheng Yu
* **Links**: [arXiv:2308.01661](https://arxiv.org/abs/2308.01661)


#### DrivingDiffusion: Layout-Guided Multi-view Driving Scenarios Video Generation with Latent Diffusion Model

* **Authors**: Xiaofan Li, Yifu Zhang, Xiaoqing Ye 
* **Links**: [ECCV 2024](https://doi.org/10.1007/978-3-031-73229-4_27) · [arXiv:2310.07771](https://arxiv.org/abs/2310.07771)

#### Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving

* **Authors**: Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, Zhaoxiang Zhang
* **Links**: [CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Driving_into_the_Future_Multiview_Visual_Forecasting_and_Planning_with_CVPR_2024_paper.pdf)


#### Vista: A generalizable driving world model with high fidelity and versatile controllability

* **Authors**: Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, Hongyang Li
* **Links**: [NeurIPS 2024](https://openreview.net/pdf?id=Tw9nfNyOMy) · [arXiv:2405.17398](https://arxiv.org/abs/2405.17398)

#### GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control

* **Authors**:Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Pedro M B Rezende, Yasaman Haghighi, David Brüggemann, Isinsu Katircioglu, Lin Zhang, Xiaoran Chen, Suman Saha, Marco Cannici, Elie Aljalbout, Botao Ye, Xi Wang, Aram Davtyan, Mathieu Salzmann, Davide Scaramuzza, Marc Pollefeys, Paolo Favaro, Alexandre Alahi
* **Links**: [CVPR 2025](https://openaccess.thecvf.com/content/CVPR2025/papers/Hassan_GEM_A_Generalizable_Ego-Vision_Multimodal_World_Model_for_Fine-Grained_Ego-Motion_CVPR_2025_paper.pdf)


#### SimGen: Simulator-conditioned Driving Scene Generation

* **Authors**: Yunsong Zhou, Michael Simon, Zhenghao Peng, Sicheng Mo, Hongzi Zhu, Minyi Guo, Bolei Zhou
* **Links**: [arXiv:2406.09386](https://arxiv.org/abs/2406.09386)

#### Simworld: A unified benchmark for simulator-conditioned scene generation via world model

* **Authors**: Xinqing Li, Ruiqi Song, Qingyu Xie, Ye Wu, Nanxin Zeng, Yunfeng Ai
* **Links**: [arXiv:2503.13952](https://arxiv.org/abs/2503.13952)

#### Bevworld: A multimodal world model for autonomous driving via unified bev latent space

* **Authors**: Yumeng Zhang, Shi Gong, Kaixin Xiong, Xiaoqing Ye, Xiaofan Li, Xiao Tan, Fan Wang, Jizhou Huang, Hua Wu, Haifeng Wang
* **Links**: [arXiv:2407.05679](https://arxiv.org/abs/2407.05679)


#### Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space

* **Authors**: Jian Zhu, Zhengyu Jia, Tian Gao, Jiaxin Deng, Shidi Li, Fu Liu, Peng Jia, Xianpeng Lang, Xiaolong Sun
* **Links**: [arXiv:2503.09215](https://arxiv.org/pdf/2503.09215)

#### Physical Informed Driving World Model

* **Authors**: Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu
* **Links**: [arXiv:2412.08410](https://arxiv.org/abs/2412.08410)

#### DriVerse: Navigation World Model for Driving Simulation via Multimodal Trajectory Prompting and Motion Alignment

* **Authors**: Xiaofan Li, Chenming Wu, Zhao Yang, Zhihao Xu, Dingkang Liang, Yumeng Zhang, Ji Wan, Jun Wang
* **Links**: [arXiv:2504.18576](https://arxiv.org/abs/2504.18576)

#### Unleashing generalization of end-to-end autonomous driving with controllable long video generation

* **Authors**: Enhui Ma, Lijun Zhou, Tao Tang, Zhan Zhang, Dong Han, Junpeng Jiang, Kun Zhan, Peng Jia, Xianpeng Lang, Haiyang Sun, Di Lin, Kaicheng Yu
* **Links**: [arXiv:2406.01349](https://arxiv.org/abs/2406.01349)

#### Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception (UniFuture)  
* **Authors**: Dingkang Liang, Dingyuan Zhang, Xin Zhou, Sifan Tu, Tianrui Feng, Xiaofan Li, Yumeng Zhang, Mingyang Du, Xiao Tan, Xiang Bai  
* **Links**: [arXiv:2503.13587](https://arxiv.org/abs/2503.13587)  

#### DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer
* **Authors**: Junpeng Jiang, Gangyi Hong, Miao Zhang, Hengtong Hu, Kun Zhan, Rui Shao, Liqiang Nie  
* **Links**: [arXiv:2504.19614](https://arxiv.org/abs/2504.19614)  

#### CoGen: 3D Consistent Video Generation via Adaptive Conditioning for Autonomous Driving  
* **Authors**: Yishen Ji, Ziyue Zhu, Zhenxin Zhu, Kaixin Xiong, Ming Lu, Zhiqi Li, Lijun Zhou, Haiyang Sun, Bing Wang, Tong Lu  
* **Links**: [arXiv:2503.22231](https://arxiv.org/abs/2503.22231)  

#### GAIA-1: A Generative World Model for Autonomous Driving

* **Authors**: Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, Gianluca Corrado
* **Links**: [arXiv:2309.17080](https://arxiv.org/abs/2309.17080)

#### GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving  
* **Authors**: Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, Gianluca Corrado 
* **Links**: [arXiv:2503.20523](https://arxiv.org/abs/2503.20523)  

#### MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving 
* **Authors**: Haiguang Wang, Daqi Liu, Hongwei Xie, Haisong Liu, Enhui Ma, Kaicheng Yu, Limin Wang, Bing Wang  
* **Links**: [arXiv:2503.15875](https://arxiv.org/abs/2503.15875)  

#### HoloDrive: Holistic 2D-3D Multi-Modal Street Scene Generation for Autonomous Driving
* **Authors**: Zehuan Wu, Jingcheng Ni, Xiaodong Wang, Yuxin Guo, Rui Chen, Lewei Lu, Jifeng Dai, Yuwen Xiong  
* **Links**: [arXiv:2412.01407](https://arxiv.org/abs/2412.01407)  

#### Street-View Image Generation from a Bird's-Eye View Layout
  
* **Authors**: Alexander Swerdlow, Runsheng Xu, Bolei Zhou  
* **Links**: [arXiv:2301.04634](https://arxiv.org/abs/2301.04634) 


#### DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT
* **Authors**: Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Ping Tan
* **Links**: [arXiv:2412.19505](https://arxiv.org/abs/2412.19505)  

#### Probing multimodal llms as world models for driving

* **Authors**: Shiva Sreeram, Tsun-Hsuan Wang, Alaa Maalouf, Guy Rosman, Sertac Karaman, Daniela Rus
* **Links**: [arXiv:2405.05956](https://arxiv.org/abs/2405.05956)  

#### Mitigating covariate shift in imitation learning for autonomous vehicles using latent space generative world models

* **Authors**: Alexander Popov, Alperen Degirmenci, David Wehr, Shashank Hegde, Ryan Oldja, Alexey Kamenev, Bertrand Douillard, David Nistér, Urs Muller, Ruchi Bhargava, Stan Birchfield, Nikolai Smolyanskiy
* **Links**: [arXiv:2409.16663](https://arxiv.org/abs/2409.16663)  

#### Imagine-2-Drive: High-Fidelity World Modeling in CARLA for Autonomous Vehicles

* **Authors**: Anant Garg, K Madhava Krishna
* **Links**: [arXiv:2411.10171](https://arxiv.org/abs/2411.10171)

#### MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction

* **Authors**:Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, Zehuan Wu
* **Links**: [arXiv:2502.11663](https://arxiv.org/abs/2502.11663)

#### DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation

* **Authors**: Jiazhe Guo, Yikang Ding, Xiwu Chen, Shuo Chen, Bohan Li, Yingshuang Zou, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Zhiheng Li, Hao Zhao  
* **Links**: [arXiv:2503.15208](https://arxiv.org/abs/2503.15208)  

#### Neural Volumetric World Models for Autonomous Driving (NeMo)

* **Authors**: Zanming Huang, Jimuyang Zhang, Eshed Ohn-Bar
* **Links**: [ECCV 2024](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02571.pdf)

